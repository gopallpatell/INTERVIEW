ğŸŸ¦ğŸŸ© BLUE-GREEN DEPLOYMENT (REAL-TIME WORKING)
Scenario

App name: myapp
Current version (LIVE): v1 (Blue)
New version to release: v2 (Green)
Goal: Zero downtime + instant rollback

ğŸ§  Core idea (before YAML)
Two deployments run at the same time
A Service decides which one gets traffic
Switching traffic = changing a label selector
No pod restarts, no downtime

STEP 0ï¸âƒ£ Namespace (isolation â€“ production practice)
apiVersion: v1
kind: Namespace
metadata:
  name: prod

kubectl apply -f namespace.yaml


ğŸ”¹ What happens
Kubernetes creates an isolated environment
All prod resources live here

STEP 1ï¸âƒ£ Deploy BLUE (current live version)
blue-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
  namespace: prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: myapp
        image: myapp:v1
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

kubectl apply -f blue-deployment.yaml


ğŸ”¹ What happens internally
Deployment creates a ReplicaSet
ReplicaSet creates 3 Pods
Pods labeled: app=myapp, version=blue
Readness probe ensures traffic goes only to healthy pods

STEP 2ï¸âƒ£ Create Service (routes traffic to BLUE)
service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  namespace: prod
spec:
  type: ClusterIP
  selector:
    app: myapp
    version: blue
  ports:
  - port: 80
    targetPort: 80

kubectl apply -f service.yaml


ğŸ”¹ What happens
Service selects only blue pods
Traffic flow:
User â†’ Service â†’ Blue Pods (v1)


ğŸŸ¦ Blue is LIVE

STEP 3ï¸âƒ£ Deploy GREEN (new version, no traffic yet)
green-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-green
  namespace: prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: myapp
        image: myapp:v2
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

kubectl apply -f green-deployment.yaml


ğŸ”¹ What happens
Green pods are created
They run in parallel
Service still points to Blue
Users donâ€™t see Green

ğŸŸ¢ Green is READY but hidden

STEP 4ï¸âƒ£ Test GREEN (real-time validation)
Port-forward for testing
kubectl port-forward deploy/myapp-green 8081:80 -n prod

Open:
http://localhost:8081


ğŸ”¹ What you check
App loads
APIs respond
Logs are clean
DB migrations OK

ğŸ’¡ This is what QA & DevOps do before release

STEP 5ï¸âƒ£ TRAFFIC SWITCH (THE REAL MAGIC) ğŸ”„
Now comes the actual deployment moment.
Update service.yaml
spec:
  selector:
    app: myapp
    version: green

kubectl apply -f service.yaml

ğŸ”¹ What happens in milliseconds
Service stops sending traffic to Blue
Service starts sending traffic to Green
No pod restarts
No connection drops
User â†’ Service â†’ Green Pods (v2)

ğŸŸ¢ Green is LIVE
ğŸŸ¦ Blue is idle but still running

STEP 6ï¸âƒ£ Monitoring (real production step)
kubectl get pods -n prod
kubectl logs -l version=green -n prod
kubectl describe svc myapp-service -n prod


ğŸ”¹ What SRE watches
Error rate
Latency
CPU/memory
Logs

STEP 7ï¸âƒ£ ROLLBACK (instant, no rebuild)
If something goes wrong ğŸš¨
Switch back to BLUE
spec:
  selector:
    app: myapp
    version: blue

kubectl apply -f service.yaml

ğŸ”¹ What happens
Traffic instantly goes back
Green pods stay (for debugging)
Zero downtime rollback

STEP 8ï¸âƒ£ Cleanup (after confidence)
Once Green is stable
kubectl delete deployment myapp-blue -n prod

ğŸ”¹ What happens
Blue pods terminate
Green becomes the new baseline
REAL-WORLD PRODUCTION FLOW (SUMMARY)
Deploy Blue â†’ Live
Deploy Green â†’ Hidden
Test Green
Switch Service â†’ Green
Monitor
Delete Blue

WHY COMPANIES LOVE BLUE-GREEN
âœ… Zero downtime
âœ… Instant rollback
âœ… No risky rolling restarts
âœ… Easy to automate (CI/CD, ArgoCD, Helm)

ONE-LINE FINAL SUMMARY
Blue-Green deployment runs two app versions in parallel and switches traffic instantly using a Service selector, 
giving zero-downtime releases and immediate rollback.

If you want next, I can:

Convert this to Ingress-based Blue-Green

Show ArgoCD Blue-Green

Compare Blue-Green vs Canary

Add database migration safety



PHASE 1: Requirement Understanding (VERY IMPORTANT)
Step 1ï¸âƒ£ Understand Application

You ask developers:

How many services?

Backend language?

Database?

Expected users?

Security requirements (HIPAA-like)?

Example services:

auth-service

appointment-service

patient-record-service

Step 2ï¸âƒ£ Decide Architecture

You decide:

Cloud â†’ AWS

Containerization â†’ Docker

Orchestration â†’ EKS

CI/CD â†’ CodePipeline

Infra â†’ Terraform

Logs â†’ ELK Stack

ğŸ§© PHASE 2: AWS Account & Security Setup
Step 3ï¸âƒ£ Create IAM Users & Roles

You create:

IAM user for DevOps

IAM role for EC2

IAM role for EKS nodes

IAM role for CodePipeline

âœ… Principle: Least Privilege

Step 4ï¸âƒ£ Setup VPC (Networking)

Using Terraform:

Create VPC

Create public & private subnets

Internet Gateway

NAT Gateway

ğŸ“Œ EKS & RDS go in private subnet

ğŸ§© PHASE 3: Infrastructure as Code (Terraform)
Step 5ï¸âƒ£ Terraform Project Structure
terraform/
 â”œâ”€â”€ vpc.tf
 â”œâ”€â”€ eks.tf
 â”œâ”€â”€ rds.tf
 â”œâ”€â”€ s3.tf
 â”œâ”€â”€ iam.tf
 â”œâ”€â”€ variables.tf
 â””â”€â”€ outputs.tf

Step 6ï¸âƒ£ Create S3 for Patient Files

Enable encryption

Enable versioning

Block public access

ğŸ“Œ Used for:

Medical reports

Prescriptions

Scans

Step 7ï¸âƒ£ Create RDS Database

Engine: MySQL / PostgreSQL

Enable encryption

Private subnet

Backup enabled

ğŸ“Œ Stores:

Patient info

Appointments

Doctor schedules

ğŸ§© PHASE 4: Dockerization (Application Level)
Step 8ï¸âƒ£ Create Dockerfile (Example)
FROM openjdk:17
COPY app.jar app.jar
CMD ["java","-jar","app.jar"]

Step 9ï¸âƒ£ Build & Push Image
docker build -t appointment-service .
docker tag appointment-service:latest <ECR_URL>
docker push <ECR_URL>


ğŸ“Œ Image stored in ECR

ğŸ§© PHASE 5: Kubernetes (EKS)
Step ğŸ”Ÿ Create EKS Cluster

Using Terraform:

Control plane

Worker nodes

IAM roles

Node groups

Step 1ï¸âƒ£1ï¸âƒ£ Deploy Application to EKS
Deployment YAML
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: appointment
        image: ECR_IMAGE

Step 1ï¸âƒ£2ï¸âƒ£ Expose via Service
kind: Service
type: LoadBalancer


ğŸ“Œ Users access via ALB

ğŸ§© PHASE 6: CI/CD Pipeline (AWS CodePipeline)
Step 1ï¸âƒ£3ï¸âƒ£ CI/CD Flow
Code Commit
 â†“
CodeBuild (build + docker)
 â†“
Push to ECR
 â†“
Deploy to EKS

Step 1ï¸âƒ£4ï¸âƒ£ Setup CodeBuild

Install Docker

Build image

Push to ECR

Update Kubernetes deployment

Step 1ï¸âƒ£5ï¸âƒ£ Automate Deployment

Every code change â†’ automatic deployment
âœ… Zero manual steps

ğŸ§© PHASE 7: Logging with ELK Stack
Step 1ï¸âƒ£6ï¸âƒ£ Why ELK?

To:

Debug errors

Audit access

Track failures

Step 1ï¸âƒ£7ï¸âƒ£ ELK Flow
Pod logs
 â†“
Fluentd
 â†“
Logstash
 â†“
Elasticsearch
 â†“
Kibana

Step 1ï¸âƒ£8ï¸âƒ£ Kibana Dashboards

Error count

API latency

Failed logins

ğŸ“Œ Helps doctors & admins quickly detect issues

ğŸ§© PHASE 8: Monitoring with CloudWatch
Step 1ï¸âƒ£9ï¸âƒ£ Metrics Tracked

CPU / Memory

Pod restarts

RDS connections

ALB 5xx errors

Step 2ï¸âƒ£0ï¸âƒ£ CloudWatch Alarms

Example:

CPU > 80%

Error rate > threshold

Action:

SNS Email alert

Slack notification

ğŸ“Œ Proactive issue detection

ğŸ§© PHASE 9: Security & Compliance
Step 2ï¸âƒ£1ï¸âƒ£ Data Security

S3 encryption

RDS encryption

IAM roles

HTTPS only

Step 2ï¸âƒ£2ï¸âƒ£ Access Control

Only doctors see patient records

Logs stored for audits

ğŸ§© PHASE 10: Day-to-Day DevOps Work
Step 2ï¸âƒ£3ï¸âƒ£ Daily Tasks

Monitor dashboards

Check logs

Fix failed pipelines

Scale pods

Rotate secrets
